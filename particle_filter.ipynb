{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2020 Google LLC\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    https://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import scipy.linalg\n",
    "from scipy.stats import multivariate_normal, uniform\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition_matrix(dt):\n",
    "    A = np.array([[1, dt, 0.5 * (dt**2)],\n",
    "                  [0, 1,            dt],\n",
    "                  [0, 0,            1]])\n",
    "    return scipy.linalg.block_diag(A, A)\n",
    "\n",
    "observation_matrix = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "])\n",
    "\n",
    "frame_height = 480\n",
    "frame_width = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_ball(start_x, start_y, step_count):\n",
    "    # Simulation parameters\n",
    "    dt = 0.1\n",
    "    \n",
    "    # State data\n",
    "    \n",
    "    # x, vx, ax, y, vy, ay\n",
    "    ball_state = np.array([start_x, 0, 0, start_y, 0, 0])\n",
    "    cur_time = 0\n",
    "    hold_start = 0\n",
    "    \n",
    "    # State history.\n",
    "    ball_states = []\n",
    "    \n",
    "    for _ in range(step_count):\n",
    "        ball_states.append({\n",
    "            'state': ball_state,\n",
    "            'hold': 0 if hold_start is None else 1,\n",
    "        })\n",
    "        \n",
    "        # Physics\n",
    "        ball_state = transition_matrix(dt) @ ball_state\n",
    "        cur_time += dt\n",
    "        \n",
    "        # Catch/throw\n",
    "        if hold_start is not None:\n",
    "            if cur_time - hold_start > 0.5:\n",
    "                # Throw!!\n",
    "                hold_start = None\n",
    "                ball_state[4] = -500\n",
    "                ball_state[5] = 500\n",
    "                if ball_state[0] < frame_width / 2:\n",
    "                    ball_state[1] = 220\n",
    "                else:\n",
    "                    ball_state[1] = -220\n",
    "        else:\n",
    "            if ball_state[3] >= 400:\n",
    "                ball_state[1] = 0\n",
    "                ball_state[4] = 0\n",
    "                ball_state[5] = 0\n",
    "                hold_start = cur_time\n",
    "        \n",
    "    return ball_states\n",
    "\n",
    "def make_synthetic_observations(ball_statess):\n",
    "    p_spurious_observation = 0.1\n",
    "    observationss = []\n",
    "    for ball_states in ball_statess:\n",
    "        # Observe\n",
    "        real_observations = [observation_matrix @ ball_state['state'] for ball_state in ball_states]\n",
    "        spurious_observations = []\n",
    "        if random.uniform(0, 1) < p_spurious_observation:\n",
    "            spurious_observations.append(\n",
    "                np.array([frame_width, frame_height]) * np.random.uniform(size=[2]))\n",
    "        observationss.append(np.random.permutation(real_observations + spurious_observations))\n",
    "    return observationss\n",
    "\n",
    "def make_synthetic_data():\n",
    "    ball1 = make_synthetic_ball(100, 400, 500)\n",
    "    ball2 = make_synthetic_ball(500, 400, 500)\n",
    "    ball3 = make_synthetic_ball(100, 400, 420)\n",
    "    \n",
    "    ball_statess = []\n",
    "    for i in range(500):\n",
    "        ball_states = [ball1[i], ball2[i]]\n",
    "        if i >= 80:\n",
    "            ball_states.append(ball3[i - 80])\n",
    "        ball_statess.append(ball_states)\n",
    "\n",
    "    observationss = make_synthetic_observations(ball_statess)\n",
    "    \n",
    "    return observationss, ball_statess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "observationss, ball_states = make_synthetic_data()\n",
    "xs = [d[0][0] for d in observationss]\n",
    "ys = [d[0][1] for d in observationss]\n",
    "plt.scatter(xs, ys)\n",
    "plt.ylim((480, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transition(N, particles, dt, p_teleport=0.05):\n",
    "    \"\"\"\n",
    "    particles['continuous'] is [N, 6] float array\n",
    "    particles['discrete'] [N] int array\n",
    "      - 0 = freefall\n",
    "      - 1 = hold\n",
    "    \"\"\"\n",
    "        \n",
    "    # parameters\n",
    "    # TODO: make dt invariant\n",
    "    p_freefall_hold = 0.5\n",
    "    p_hold_freefall = 0.5\n",
    "    p_appear = 0.05\n",
    "    p_disappear = 0.05\n",
    "    hold_freefall_noise = multivariate_normal([0, 0, -2000, 6000], np.diag([500*500, 100*100, 200*200, 200*200]))\n",
    "    continuous_noise = multivariate_normal([0, 0, 0, 0, 0, 0], np.diag([10*10, 10*10, 10*10, 10*10, 10*10, 10*10]))\n",
    "        \n",
    "    # particles['discrete'] state transitions\n",
    "    \n",
    "    sample = np.random.uniform(size=[N])\n",
    "    freefall_hold = (particles['discrete'] == 0) & (sample < p_freefall_hold)\n",
    "    hold_freefall = (particles['discrete'] == 1) & (sample < p_hold_freefall)\n",
    "    \n",
    "    particles['discrete'][freefall_hold] = 1\n",
    "    particles['discrete'][hold_freefall] = 0\n",
    "    \n",
    "    particles['continuous'][freefall_hold, 1] = 0\n",
    "    particles['continuous'][freefall_hold, 2] = 0\n",
    "    particles['continuous'][freefall_hold, 4] = 0\n",
    "    particles['continuous'][freefall_hold, 5] = 0\n",
    "\n",
    "    hold_freefall_noise_rvs = hold_freefall_noise.rvs(size=[np.sum(hold_freefall)])\n",
    "    if len(hold_freefall_noise_rvs.shape) == 1:\n",
    "        hold_freefall_noise_rvs = hold_freefall_noise_rvs[np.newaxis, :]\n",
    "    particles['continuous'][hold_freefall, 1] = hold_freefall_noise_rvs[:, 0]\n",
    "    particles['continuous'][hold_freefall, 2] = hold_freefall_noise_rvs[:, 1]\n",
    "    particles['continuous'][hold_freefall, 4] = hold_freefall_noise_rvs[:, 2]\n",
    "    particles['continuous'][hold_freefall, 5] = hold_freefall_noise_rvs[:, 3]\n",
    "    \n",
    "    # particles['continuous'] state transitions\n",
    "    \n",
    "    particles['continuous'] = (transition_matrix(dt) @ particles['continuous'][:, :, np.newaxis])[:, :, 0]\n",
    "    particles['continuous'] += continuous_noise.rvs(size=[N])\n",
    "    \n",
    "    # appear/disappear state transitions\n",
    "    \n",
    "    particles['log_p_exist'] = logsumexp([\n",
    "        np.log(1 - p_disappear) + particles['log_p_exist'],\n",
    "        np.log(p_appear) + np.log(1 - np.exp(particles['log_p_exist']))\n",
    "    ])\n",
    "    \n",
    "    # teleportation\n",
    "    \n",
    "    N_new = int(p_teleport * N)\n",
    "    new_discrete = np.full([N_new], 1, dtype=int)\n",
    "    new_continuous = np.zeros([N_new, 6])\n",
    "    new_continuous[:, 0] = np.random.uniform(0, frame_width, size=[N_new])\n",
    "    new_continuous[:, 3] = np.random.uniform(0, frame_height, size=[N_new])\n",
    "    particles['discrete'] = np.concatenate([particles['discrete'], new_discrete])\n",
    "    particles['continuous'] = np.concatenate([particles['continuous'], new_continuous])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(N, particles, observation):\n",
    "    N_before = particles['continuous'].shape[0]\n",
    "    \n",
    "    # parameters\n",
    "    p_obs = 0.9\n",
    "    p_spurious_obs = 0.1\n",
    "    observation_noise = multivariate_normal([0, 0], np.diag([20*20, 20*20]))\n",
    "    \n",
    "    particle_observations = (observation_matrix @ particles['continuous'][:, :, np.newaxis])[:, :, 0]\n",
    "    \n",
    "    if observation is None:\n",
    "        # liklihood from the case where we failed to observe the ball\n",
    "        observation_logliklihoods = np.full([N_before], np.log(1 - p_obs))\n",
    "        logp_evidence_given_not_exists = 0\n",
    "    else:\n",
    "        observation_logliklihoods = observation_noise.logpdf(particle_observations - observation) + np.log(p_obs)\n",
    "        logp_evidence_given_not_exists = np.log(p_spurious_obs) - np.log(frame_height * frame_width)\n",
    "\n",
    "    \n",
    "    logp_evidence_given_exists = logsumexp(observation_logliklihoods - np.log(N_before))\n",
    "\n",
    "    logp_evidence = logsumexp([\n",
    "        logp_evidence_given_exists + particles['log_p_exist'],\n",
    "        logp_evidence_given_not_exists + np.log(1 - np.exp(particles['log_p_exist'])) # need preciser??\n",
    "    ])\n",
    "    particles['log_p_exist'] = logp_evidence_given_exists + particles['log_p_exist'] - logp_evidence\n",
    "    \n",
    "    indices = np.random.choice(\n",
    "        N_before, N, replace=True,\n",
    "        p=np.exp(observation_logliklihoods - logsumexp(observation_logliklihoods)))\n",
    "    \n",
    "    particles['continuous'] = particles['continuous'][indices, :]\n",
    "    particles['discrete'] = particles['discrete'][indices]\n",
    "    \n",
    "# Returns the log liklihood of `observation` given that the observation comes from the existing ball whose\n",
    "# distribution is described by `particles`.\n",
    "#\n",
    "# \"existing ball\" means that we ignore particles['log_p_exist'] for the purposes of this calculation.\n",
    "def logp_observation(particles, observation):\n",
    "    observation_noise = multivariate_normal([0, 0], np.diag([20*20, 20*20]))\n",
    "    particle_observations = (observation_matrix @ particles['continuous'][:, :, np.newaxis])[:, :, 0]\n",
    "    particle_logp_observations = observation_noise.logpdf(particle_observations - observation)\n",
    "    return logsumexp(particle_logp_observations - np.log(particles['continuous'].shape[0]))\n",
    "\n",
    "# Returns the log liklihood of `observation` given that it is spurious.\n",
    "def logp_observation_given_spurious(observation):\n",
    "    return -np.log(frame_width * frame_height)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unweighted_resample(N, particles):\n",
    "    N_before = particles['continuous'].shape[0]\n",
    "    indices = np.random.choice(N_before, N, replace=True)\n",
    "    particles['continuous'] = particles['continuous'][indices, :]\n",
    "    particles['discrete'] = particles['discrete'][indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject_nearby(particles, position, peak_size=100):\n",
    "    close_mask = np.linalg.norm(particles['continuous'][:, [0, 3]] - position, axis=1) < peak_size\n",
    "    particles['continuous'] = particles['continuous'][~close_mask, :]\n",
    "    particles['discrete'] = particles['discrete'][~close_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peak(positions, peak_size=100, peak_threshold=0.2):\n",
    "    last_candidate_peak = None\n",
    "    candidate_peak = positions[np.random.choice(positions.shape[0], 1)]\n",
    "    \n",
    "    while last_candidate_peak is None or np.linalg.norm(candidate_peak - last_candidate_peak) > 1.0:\n",
    "        last_candidate_peak = candidate_peak\n",
    "        close_mask = np.linalg.norm(positions - candidate_peak, axis=1) < peak_size\n",
    "        candidate_peak = np.mean(positions[close_mask, :], axis=0)\n",
    "        \n",
    "    close_mask = np.linalg.norm(positions - candidate_peak, axis=1) < peak_size\n",
    "    if np.sum(close_mask) < peak_threshold * positions.shape[0]:\n",
    "        return None\n",
    "    return candidate_peak\n",
    "\n",
    "def find_peaks(positions, peak_size=100, peak_threshold=0.2):\n",
    "    N = positions.shape[0]\n",
    "    peaks = []\n",
    "    for _ in range(10):\n",
    "        if positions.shape[0] < peak_threshold * N:\n",
    "            break\n",
    "        peak = find_peak(positions, peak_size=peak_size, peak_threshold=peak_threshold)\n",
    "        if peak is None:\n",
    "            continue\n",
    "        peaks.append(peak)\n",
    "        close_mask = np.linalg.norm(positions - peak, axis=1) < peak_size\n",
    "        positions = positions[~close_mask, :]\n",
    "    return peaks\n",
    "\n",
    "def find_biggest_peak(positions, peak_size=100, peak_threshold=0.2):\n",
    "    peaks = find_peaks(positions, peak_size=peak_size, peak_threshold=peak_threshold)\n",
    "    \n",
    "    biggest_size = 0\n",
    "    biggest_index = None\n",
    "    for i, peak in enumerate(peaks):\n",
    "        close_mask = np.linalg.norm(positions - peak, axis=1) < peak_size\n",
    "        size = np.sum(close_mask)\n",
    "        if size > biggest_size:\n",
    "            biggest_size = size\n",
    "            biggest_index = i\n",
    "    \n",
    "    if biggest_index is None:\n",
    "        return None\n",
    "    return peaks[biggest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_particles(N):\n",
    "    particles = {\n",
    "        'discrete': np.full([N], 1, dtype=int),\n",
    "        'continuous': np.zeros([N, 6]),\n",
    "        'log_p_exist': np.log(0.01), \n",
    "    }\n",
    "    particles['continuous'][:, 0] = np.random.uniform(0, frame_width, size=[N])\n",
    "    particles['continuous'][:, 3] = np.random.uniform(0, frame_height, size=[N])\n",
    "    return particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_state(N):\n",
    "    return {\n",
    "        'N': N,\n",
    "        'new_particles': make_particles(N),\n",
    "        'identified_particles': [],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(state, dt, observations):\n",
    "    N = state['N']\n",
    "    \n",
    "    # Greedily associate each observation with the highest-liklihood explanation for it.\n",
    "    # Possible explanations are:\n",
    "    # - spurious observation (-2) (allowed to explain multiple things)\n",
    "    # - observation of a new ball (-1)\n",
    "    # - observation of an identified ball (index in `state['identified_particles']`)\n",
    "    remaining_explanations = set(range(len(state['identified_particles']))).union([-1])\n",
    "    observation_explanations = []\n",
    "    for observation in observations:\n",
    "        best_explanation = -2\n",
    "        best_logp = logp_observation_given_spurious(observation) + np.log(0.001) #haxxxx\n",
    "        for explanation in remaining_explanations:\n",
    "            particles = state['new_particles'] if explanation == -1 else state['identified_particles'][explanation]\n",
    "            logp = logp_observation(particles, observation)\n",
    "            if logp > best_logp:\n",
    "                best_logp = logp\n",
    "                best_explanation = explanation\n",
    "        observation_explanations.append(best_explanation)\n",
    "        remaining_explanations.discard(best_explanation)\n",
    "        \n",
    "    new_particles_observation_index = None\n",
    "    identified_particle_observation_indices = [None] * len(state['identified_particles'])\n",
    "    for (observation_index, explanation) in enumerate(observation_explanations):\n",
    "        if explanation == -1:\n",
    "            new_particles_observation_index = observation_index\n",
    "        if explanation >= 0:\n",
    "            identified_particle_observation_indices[explanation] = observation_index\n",
    "        \n",
    "    biggest_peaks = []\n",
    "    for (i, particles) in enumerate(state['identified_particles']):\n",
    "        observation_index = identified_particle_observation_indices[i]\n",
    "        observation = None if observation_index is None else observations[observation_index]\n",
    "        transition(N, particles, 0.1, p_teleport=0.01)\n",
    "        resample(N, particles, observation)\n",
    "        positions = particles['continuous'][:, [0, 3]]\n",
    "        #biggest_peaks.append(np.mean(positions, axis=0))\n",
    "        peak = find_biggest_peak(positions)\n",
    "        if peak is not None:\n",
    "            biggest_peaks.append(peak)\n",
    "        \n",
    "    transition(N, state['new_particles'], 0.1, p_teleport=0.1)\n",
    "    for peak in biggest_peaks:\n",
    "        reject_nearby(state['new_particles'], peak)\n",
    "    new_particles_observation = None if new_particles_observation_index is None else observations[new_particles_observation_index]\n",
    "    resample(N, state['new_particles'], new_particles_observation)\n",
    "    \n",
    "    if state['new_particles']['log_p_exist'] > -5e-4:\n",
    "        positions = state['new_particles']['continuous'][:, [0, 3]]\n",
    "        peak = find_biggest_peak(positions)\n",
    "        if peak is not None:\n",
    "            peak_size = 100\n",
    "            close_mask = np.linalg.norm(positions - peak, axis=1) < peak_size\n",
    "            state['new_particles']['continuous'] = state['new_particles']['continuous'][close_mask, :]\n",
    "            state['new_particles']['discrete'] = state['new_particles']['discrete'][close_mask]\n",
    "            unweighted_resample(N, state['new_particles'])\n",
    "            state['identified_particles'].append(state['new_particles'])\n",
    "            state['new_particles'] = make_particles(N)\n",
    "            \n",
    "    state['identified_particles'] = [p for p in state['identified_particles'] if p['log_p_exist'] > -4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20_000\n",
    "state = make_state(N)\n",
    "for time_index in range(10):\n",
    "    observations = observationss[time_index]\n",
    "    step(state, 0.1, observations)\n",
    "    \n",
    "    print(\"new particle log_p_exist\", state['new_particles']['log_p_exist'])\n",
    "    \n",
    "    if time_index > 100:\n",
    "        plt.xlim([0, frame_width])\n",
    "        plt.ylim([frame_height, 0])\n",
    "        for p in state['identified_particles']:\n",
    "            plt.scatter(p['continuous'][:, [0]], p['continuous'][:, [3]])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on some images!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import PIL.ImageOps\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "import train\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = train.UNet()\n",
    "net.load_state_dict(torch.load('net03'))\n",
    "net.train(False)\n",
    "device = torch.device('cuda')\n",
    "net = net.to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_balls(pred):\n",
    "    balls = []\n",
    "\n",
    "    while True:\n",
    "        a = torch.nn.functional.conv2d(pred.unsqueeze(0).unsqueeze(0), torch.ones(1, 1, 20, 20).to(device=device)).squeeze(0).squeeze(0)\n",
    "        if torch.max(a) < 0.6 * 20 * 20:\n",
    "            break\n",
    "        max_index = torch.argmax(a)\n",
    "        max_x = max_index % a.shape[1] + 10\n",
    "        max_y = max_index // a.shape[1] + 10\n",
    "        pred[max_y-20:max_y+20, max_x-20:max_x+20] = 0\n",
    "        balls.append(np.array([max_x.item(), max_y.item()]))\n",
    "\n",
    "    return balls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observationss = []\n",
    "for i in range(200):\n",
    "    with torch.no_grad():\n",
    "        img = Image.open('data/cap3/img/%03d.png' % i)\n",
    "        img = torch.from_numpy(np.array(img).transpose((2, 0, 1))).type(torch.FloatTensor).to(device=device)\n",
    "        pred = net(img.unsqueeze(0)).squeeze(0).squeeze(0)\n",
    "        pred = sigmoid(pred)\n",
    "    observationss.append([np.array(x) for x in find_balls(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20_000\n",
    "state = make_state(N)\n",
    "for time_index in range(200):\n",
    "    observations = observationss[time_index]\n",
    "    step(state, 0.1, observations)\n",
    "        \n",
    "    plt.xlim([0, frame_width])\n",
    "    plt.ylim([frame_height, 0])\n",
    "    img = Image.open('data/cap3/img/%03d.png' % time_index)\n",
    "    plt.imshow(img)\n",
    "    for p in state['identified_particles']:\n",
    "        plt.scatter(p['continuous'][:, [0]], p['continuous'][:, [3]])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "xres = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "yres = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "cur_time = time.time()\n",
    "\n",
    "N = 20_000\n",
    "state = make_state(N)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret: break\n",
    "        \n",
    "    next_time = time.time()\n",
    "    dt = next_time - cur_time\n",
    "    cur_time = next_time\n",
    "    print(dt)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        cv2.imwrite('tmp.png', frame)\n",
    "        img = Image.open('tmp.png')\n",
    "        img = torch.from_numpy(np.array(img).transpose((2, 0, 1))).type(torch.FloatTensor).to(device=device)\n",
    "        pred = net(img.unsqueeze(0)).squeeze(0).squeeze(0)\n",
    "        pred = sigmoid(pred)\n",
    "        \n",
    "    balls = find_balls(pred)\n",
    "    step(state, dt, balls)\n",
    "    \n",
    "    for ball in balls:\n",
    "        cv2.circle(frame, (ball[0], ball[1]), 10, (0, 0, 255))\n",
    "\n",
    "    print(state['new_particles']['log_p_exist'])\n",
    "    for p in state['identified_particles']:\n",
    "        print(\"p_hold: \", np.mean(p['discrete']))\n",
    "\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "    for i, p in enumerate(state['identified_particles']):\n",
    "        color = colors[i % len(colors)]\n",
    "        for bs in p['continuous']:\n",
    "            cv2.circle(frame, (int(bs[0]), int(bs[3])), 4, color)\n",
    "    \n",
    "    cv2.imshow('frame', frame)\n",
    "    cv2.imshow('pred', pred.cpu().numpy())\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): break\n",
    "        \n",
    "    print()\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logsumexp([1, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
